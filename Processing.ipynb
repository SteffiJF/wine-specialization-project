{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Processing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMs/nEO2tG77VMEFN8OtNKS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SteffiJF/wine-specialization-project/blob/main/Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjCvVK-FSBxs"
      },
      "source": [
        "Processing the data and making two files, one with aggregated sales per district and one with the top five wines per district"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uXzftsEhkNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a16ab286-2b7e-4cd1-9a53-c3933b864bb6"
      },
      "source": [
        "!pip install \"dask[dataframe]\" \n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import dask.dataframe as dd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import drive\n",
        "from oauth2client.client import GoogleCredentials\t\n",
        "from datetime import datetime\n",
        "%matplotlib inline\n",
        "from pandas import DataFrame\n",
        "from pandas import Series\n",
        "from pandas import concat\n",
        "from pandas import read_csv\n",
        "from datetime import datetime\n",
        "from matplotlib import pyplot\n",
        "from numpy import array\n",
        "import seaborn as sns\n",
        "import matplotlib._color_data as mcd\n",
        "\n",
        "# Use seaborn style defaults and set the default figure size\n",
        "sns.set(rc={'figure.figsize':(10, 5)})"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dask[dataframe] in /usr/local/lib/python3.6/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.13.0; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[dataframe]) (1.19.5)\n",
            "Requirement already satisfied: toolz>=0.7.3; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[dataframe]) (0.11.1)\n",
            "Collecting partd>=0.3.10; extra == \"dataframe\"\n",
            "  Downloading https://files.pythonhosted.org/packages/44/e1/68dbe731c9c067655bff1eca5b7d40c20ca4b23fd5ec9f3d17e201a6f36b/partd-1.1.0-py3-none-any.whl\n",
            "Collecting fsspec>=0.6.0; extra == \"dataframe\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/80/72ac0982cc833945fada4b76c52f0f65435ba4d53bc9317d1c70b5f7e7d5/fsspec-0.8.5-py3-none-any.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.23.0; extra == \"dataframe\" in /usr/local/lib/python3.6/dist-packages (from dask[dataframe]) (1.1.5)\n",
            "Collecting locket\n",
            "  Downloading https://files.pythonhosted.org/packages/50/b8/e789e45b9b9c2db75e9d9e6ceb022c8d1d7e49b2c085ce8c05600f90a96b/locket-0.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0; extra == \"dataframe\"->dask[dataframe]) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.0; extra == \"dataframe\"->dask[dataframe]) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.23.0; extra == \"dataframe\"->dask[dataframe]) (1.15.0)\n",
            "Installing collected packages: locket, partd, fsspec\n",
            "Successfully installed fsspec-0.8.5 locket-0.2.1 partd-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpSG_kXn8Sr5"
      },
      "source": [
        "Downloading data from Google Disk (you need to upload the files into your own Google Disk and update to your own id or choose an alternative method to load data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqxlCA5T8dv8"
      },
      "source": [
        "#Settting up connection to Google Disk\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#Downloading sales data\n",
        "download = drive.CreateFile({'id': '16d4KjwOcHkZJaBp4C1w1PhFHuInIRuu3'})\n",
        "download.GetContentFile('Salg.csv')\n",
        "sales = pd.read_csv(\"Salg.csv\", sep=';',  parse_dates = [['År', 'Måned']], low_memory=False)\n",
        "\n",
        "#Downloading ranking data\n",
        "download2 = drive.CreateFile({'id': '1rtnYjvN7q8reK-RCX3H_7qVYW_837t9J'})\n",
        "download2.GetContentFile('Rangering.csv')\n",
        "ranks = pd.read_csv(\"Rangering.csv\", sep=';',  parse_dates = [['År', 'Måned']], low_memory=False)\n",
        "\n",
        "#Downloading product data\n",
        "download3 = drive.CreateFile({'id': '1s8afH0Xt_YqCpMvCuWkUeI9T0RJVuT32'})\n",
        "download3.GetContentFile('Products.csv')\n",
        "products = pd.read_csv(\"Products.csv\", sep=';', low_memory=False)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JltDADt9rYsg"
      },
      "source": [
        "Setting up data for modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCnnJ3U1P4V7"
      },
      "source": [
        "wineSales = sales[sales['Varetype']=='Rødvin']\n",
        "countries = wineSales['Land'].tolist()\n",
        "countries = list(dict.fromkeys(countries))\n",
        "\n",
        "totalLiters=wineSales.groupby(['Land', 'Distrikt'])['Liter denne måned i år'].sum()\n",
        "totalLiters=totalLiters.to_dict()\n",
        "\n",
        "\n",
        "#Making a list of tuples containing country and district, only taking the\n",
        "#districts with more than 50000 liters sold in total\n",
        "areas=[]\n",
        "for c in countries:\n",
        "  dfCountry = sales[sales['Land']== c]\n",
        "  districts = sales['Distrikt'].tolist()\n",
        "  districts = list(dict.fromkeys(districts))\n",
        "  for d in districts:\n",
        "    if (c,d) in totalLiters:\n",
        "      if not pd.isnull(d):\n",
        "        if totalLiters[c,d]>=50000:\n",
        "          if d != 'ZZZ' and d!='RHO':\n",
        "            area = (c,d)\n",
        "            areas.append(area)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlYZUbhBUa6o"
      },
      "source": [
        "Constructing two new dataframes (aggregated sales per district and top 5 wines)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nb-Hc7zwrifG"
      },
      "source": [
        "#Selecting area and features to study\n",
        "\n",
        "country = 'ITA'  \n",
        "district = 'PIE' \n",
        "product = 'Rødvin'\n",
        "packaging = 'Engangsflaske av glass'\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "# Merging files into single dataframe\n",
        "def merging(sales, ranks, products, country, district, product, packaging):\n",
        "\n",
        "  #Removing irrelevant areas\n",
        "  chosenSales = sales[sales['Land']==country]\n",
        "  chosenSales = chosenSales[chosenSales['Distrikt']==district]\n",
        "  chosenSales = chosenSales[chosenSales['Varetype']==product]\n",
        "\n",
        "  #Changing column names to match data frames\n",
        "  products = products.rename(columns={\"VMP ID\": \"Artikkelnr\", \"Alkohol %\": \"Alkoholprosent\"})\n",
        "\n",
        "  #Keeping interesting columns\n",
        "  smallSales = chosenSales[['År_Måned','Artikkelnr','Liter denne måned i år', 'Salgspris', 'Årgang', 'Volum', 'Alkoholprosent', 'Utvalg']]\n",
        "  smallRanks = ranks[['År_Måned','Artikkelnr', 'Rangering', 'Status',\t'Styringstall']]\n",
        "  smallProducts = products[['Artikkelnr', 'Årgang', 'Alkoholprosent', 'Emballasjetype']]\n",
        "\n",
        "  #Converting to dask dataframe for merging\n",
        "  dfSales = dd.from_pandas(smallSales, npartitions=10)\n",
        "  dfRanks = dd.from_pandas(smallRanks, npartitions=10)\n",
        "  dfProducts = dd.from_pandas(smallProducts, npartitions=10)\n",
        "\n",
        "  # Merge the csv files.\n",
        "  df = dd.merge(dfSales, dfRanks, how='left', on=['Artikkelnr', 'År_Måned'])\n",
        "  df = dd.merge(df, dfProducts, how='left', on=['Artikkelnr','Årgang'])\n",
        "\n",
        "  #Converting back to pandas\n",
        "  df=df.compute()\n",
        "\n",
        "  #Sorting data\n",
        "  df=df.sort_values(by=['År_Måned', 'Artikkelnr', 'Årgang'])\n",
        "  \n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "#Adding relevant columns and removing irrelevant rows\n",
        "def column_preparation(df):\n",
        "  \n",
        "  #Removing articles with wrong packaging\n",
        "  df = df[df['Emballasjetype']==packaging]\n",
        "  \n",
        "  #Summing up total amount sold per article nr and adding as column\n",
        "  totalLiters=df.groupby(['Artikkelnr'])['Liter denne måned i år'].sum()\n",
        "  totalLiters=totalLiters.to_dict()\n",
        "  df = df.assign(Total=df['Artikkelnr'].map(totalLiters))\n",
        "  \n",
        "  #Using Alkoholprosent_x if not NaN, otherwise using Alkoholprosent_y\n",
        "  df = df.assign(Alkoholprosent = np.where(df['Alkoholprosent_x'].notnull(), df['Alkoholprosent_x'], df['Alkoholprosent_y']) )\n",
        "  df = df.drop(['Alkoholprosent_x','Alkoholprosent_y'], axis='columns')\n",
        "\n",
        "  #Adding a column for amount of alcohol divided by sales price\n",
        "  df['Alk/Pris'] = df['Alkoholprosent']*df['Volum']/df['Salgspris']  \n",
        "\n",
        "  #Adding a column for volume divided by sales price\n",
        "  df['Vol/Pris'] = df['Volum']/df['Salgspris']\n",
        "\n",
        "  #Changing name of column to avoid space\n",
        "  df = df.rename(columns={'Liter denne måned i år': 'Liter'})\n",
        "\n",
        "  #Removing duplicates\n",
        "  df = df.drop_duplicates()\n",
        "\n",
        "  #Removing aritcles that have only 12 rows or less \n",
        "  df = df[df.groupby('Artikkelnr').Artikkelnr.transform(len) > 12]\n",
        "\n",
        "  #Removing articles with zero total sales\n",
        "  df = df[df['Total']>1000]\n",
        "\n",
        "  #Removing articles with nan total sales\n",
        "  df = df[df['Total']!=np.nan]\n",
        "  \n",
        "  return df\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "#Handling duplicates\n",
        "\n",
        "#Finding dates that are duplicated in list of dates and returning\n",
        "#a string that can be used as index for dataframe\n",
        "def dupDates(dates):\n",
        "  seen = {}\n",
        "  dupes = []\n",
        "\n",
        "  for i in dates:\n",
        "      if i not in seen:\n",
        "          seen[i] = 1\n",
        "      else:\n",
        "          if seen[i] == 1:\n",
        "              dupes.append(i)\n",
        "          seen[i] += 1\n",
        "  for i in range(0,len(dupes)):\n",
        "    dupes[i]=str(dupes[i])\n",
        "    dupes[i]=dupes[i].replace('-01T00:00:00.000000000','')\n",
        "  return dupes\n",
        "\n",
        "#Finds the first duplicate and returns article number and date.\n",
        "#Helps initiate new dataframe containing only duplicated rows\n",
        "def arbitrary_duplicate(df2, articles):\n",
        "  if articles:\n",
        "    for i in articles:\n",
        "      dates=df2.loc[i].index.values\n",
        "      if len(dates)-len(np.unique(dates))!=0:\n",
        "        dupes=dupDates(dates)\n",
        "        return i, dupes[0]\n",
        "  return 0, 0\n",
        "\n",
        "\n",
        "#Finding articlenrs with duplicates and returning a tuple\n",
        "#with duplicated article number and date\n",
        "def duplicate_index(df2,dfDupes, articles):\n",
        "  dupRows= []\n",
        "  for i in articles:\n",
        "    dates=df2.loc[i].index.values\n",
        "    if len(dates)-len(np.unique(dates))!=0:\n",
        "      dupes=dupDates(dates)\n",
        "      for j in range(0,len(dupes)):\n",
        "        dfDupes=dfDupes.append(df2.loc[i][dupes[j]])\n",
        "        d=(i,dupes[j])\n",
        "        dupRows.append(d)\n",
        "  return dupRows \n",
        "\n",
        "\n",
        "#Identifying the columns that vary in the duplicated rows\n",
        "def nonduplicated_columns(df):\n",
        "    my_cols = []\n",
        "    for col in df.columns:\n",
        "        if df[col].nunique(dropna=False) > 1:\n",
        "            my_cols.append(list(df.columns).index(col))\n",
        "    return my_cols\n",
        "\n",
        "#Printing the columns and amount of times that column is the\n",
        "#reason for duplicated rows\n",
        "def duplicates():\n",
        "  duplicatedColumns=np.zeros(len(df2.columns))\n",
        "\n",
        "  for art,date in dupRows:\n",
        "    #print(art, date)\n",
        "    dupCols=nonduplicated_columns(df2.loc[art][date])\n",
        "    for i in range(0,len(dupCols)):\n",
        "      duplicatedColumns[dupCols[i]]+=1\n",
        "\n",
        "  for i in range(0,len(duplicatedColumns)):\n",
        "    print(df2.columns[i],duplicatedColumns[i])\n",
        "\n",
        "#Duplicated rows caused by different amounts of liter sold per month is often \n",
        "#caused by multiple distributors, these rows will be handled by adding the Liter \n",
        "#columns to one row and removing the rest of the duplicated rows\n",
        "\n",
        "\n",
        "def remove_duplicates(df, df2, dupRows):\n",
        "  for art,date in dupRows:\n",
        "      dupCols=nonduplicated_columns(df2.loc[art][date])\n",
        "      indices=df.index[(df['Artikkelnr'] == art) & (df['År_Måned'] == date)].tolist()\n",
        "      if 0 in dupCols:\n",
        "        df.loc[indices[0],'Liter']+=df.loc[indices[1]][2]\n",
        "        df=df.drop(indices[1])\n",
        "      else:\n",
        "        for i in range(1,len(indices)):\n",
        "          df=df.drop(indices[i])\n",
        "  return df\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "#Finding difference with lag=12\n",
        "\n",
        "#Finding difference by looping through articles.\n",
        "#Is a bit slow, but doesn't take more than a few minutes. \n",
        "\n",
        "def differencing(df2,articles):\n",
        "  #Making new column\n",
        "  df2['Differanse']=0\n",
        "  df2['Prosentdifferanse']=0\n",
        "\n",
        "  #Looping through articles \n",
        "  for i in articles:\n",
        "    d=df2.loc[i]['Liter'].diff(periods=12)\n",
        "    pd=df2.loc[i]['Liter'].pct_change(periods=12)\n",
        "    dates=df2.loc[i].index.values\n",
        "    for j in range(0,len(dates)):\n",
        "      df2.loc[(i, dates[j]),'Differanse']=d[dates[j]]\n",
        "      df2.loc[(i, dates[j]),'Prosentdifferanse']=pd[dates[j]]\n",
        "       \n",
        "  return (df2.replace([np.inf, -np.inf], np.nan))\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "#Pivoting\n",
        "\n",
        "def pivoting(df):\n",
        "  pivot = df.pivot_table(index='År_Måned', columns='Artikkelnr', aggfunc='mean' )\n",
        "  pivot = pivot.reorder_levels([1,0],axis=1)\n",
        "  pivot = pivot.sort_index(axis=1, ascending=True)\n",
        "  return pivot\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "def main_preparation(sales, ranks, products, country, district, product, packaging):\n",
        "  df = merging(sales, ranks, products, country, district, product, packaging)\n",
        "  df=column_preparation(df)\n",
        "\n",
        "  #Preparations for analysis\n",
        "  \n",
        "  #List of article numbers\n",
        "  articles=df['Artikkelnr'].tolist()\n",
        "  articles=list(dict.fromkeys(articles))\n",
        "\n",
        "  #List of dates, so far in wrong order\n",
        "  dates=df['År_Måned'].tolist()\n",
        "  dates=list(dict.fromkeys(dates))\n",
        "\n",
        "  #Resetting index to remove problem of rows having same index\n",
        "  df = df.reset_index(drop=True)\n",
        "  \n",
        "  #Making a copy of the dataframe with multiindex\n",
        "  df2 = df.set_index(['Artikkelnr','År_Måned']).sort_index()\n",
        "\n",
        "  #Label encoding columns to avoid pivoting issues and storing the codes in dictionaries\n",
        "\n",
        "  #Selection\n",
        "  df['Utvalg_c']= df.Utvalg.astype(\"category\").cat.codes\n",
        "  cUtvalg = df.Utvalg.astype('category')\n",
        "  dUtvalg = dict(enumerate(cUtvalg.cat.categories))\n",
        "  \n",
        "  #Setting up new dataframe for duplicated rows, starting with arbitrary duplicate\n",
        "  #print(country, district)\n",
        "  dart, ddate = arbitrary_duplicate(df2, articles)\n",
        "\n",
        "  if dart != 0:\n",
        "    dfDupes=df2.loc[dart][ddate]\n",
        "    \n",
        "    #Tuples with duplicate articles and dates  \n",
        "    dupRows=duplicate_index(df2,dfDupes, articles)\n",
        "    \n",
        "    #Removing duplicates\n",
        "    df= remove_duplicates(df, df2, dupRows)\n",
        "  \n",
        "  #Making a copy of the dataframe with multiindex\n",
        "  df2 = df.set_index(['Artikkelnr','År_Måned']).sort_index()\n",
        "  \n",
        "  #Finding difference and percentage difference\n",
        "  #df2 = differencing(df2, articles)\n",
        "  \n",
        "  #Making a pivoted copy of df for plotting\n",
        "  pivot = pivoting(df2.reset_index())\n",
        "  \n",
        "\n",
        "  return df, df2, pivot, articles, dates\n",
        "\n",
        "\n",
        "#Sums up the wine sales for each month\n",
        "\n",
        "def aggregate(column,articles,pivot):\n",
        "  agg=0\n",
        "  for i in articles:\n",
        "    agg+=pivot[i][column].fillna(0)\n",
        "  return agg\n",
        "\n",
        "\n",
        "#Returns a dataframe only consisting of the top 5 wines\n",
        "\n",
        "def top_wines(articles,df,area):\n",
        "  #Constructing 2D list with total sales for each wine in this district\n",
        "  artTot = [[0 for i in range(2)] for j in range(len(articles))]\n",
        "\n",
        "  for i in range(0,len(articles)):\n",
        "    artTot[i][0] = articles[i]\n",
        "    artTot[i][1] = df[df['Artikkelnr'] == articles[i]].Total.values[0]\n",
        "\n",
        "  #Sorting the values in the second column highest to lowest and keeping top n\n",
        "  artTot=sorted(artTot,key=lambda x: x[1],reverse=True)[:5]\n",
        "\n",
        "  dfPopDist = pd.DataFrame()\n",
        "\n",
        "  if len(artTot)==5:\n",
        "    #Making a dataframe consisting of only top 5 wines\n",
        "    dfPopDist = df[(df['Artikkelnr'] == artTot[0][0]) |\n",
        "              (df['Artikkelnr'] == artTot[1][0]) |\n",
        "              (df['Artikkelnr'] == artTot[2][0]) |\n",
        "              (df['Artikkelnr'] == artTot[3][0]) | \n",
        "              (df['Artikkelnr'] == artTot[4][0])]\n",
        "\n",
        "    #Adding columns with district and country\n",
        "    dfPopDist.insert(2,'Område', str(area))\n",
        "\n",
        "  return dfPopDist\n",
        "\n",
        "#df, df2, pivot, articles, dates=main_preparation(sales, ranks, products, country, district, product, packaging)\n",
        "\n",
        "#Looping through each area to make two separate dataframes, one containing the \n",
        "#aggreagated liters sold per month per district and another to include the liters \n",
        "#sold per month for the five most popular wines per district  \n",
        "\n",
        "dfAgg = pd.DataFrame()\n",
        "dfPop = pd.DataFrame()\n",
        "for i in range(0,len(areas)):\n",
        "  df, df2, pivot, articles, dates=main_preparation(sales, ranks, products, areas[i][0], areas[i][1], product, packaging)\n",
        "  \n",
        "  #Aggregating the sales per month and adding new columns for district\n",
        "  aggLiter = aggregate('Liter',articles,pivot)\n",
        "  dfAgg[areas[i]] = aggLiter\n",
        "\n",
        "  #Finding the top 5 wines for each district and adding them to a dataframe\n",
        "  dfPopDist =  top_wines(articles,df,areas[i])\n",
        "  dfPop = dfPop.append(dfPopDist)\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDLQ8eTkUquq"
      },
      "source": [
        "Converting dataframes to .csv files for later analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLJ6uRdeQ3IU"
      },
      "source": [
        "dfPop.to_csv('dfTopWines.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCJ8J5NzErFW"
      },
      "source": [
        "dfAgg.to_csv('dfAgg2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}